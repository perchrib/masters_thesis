Training_log - 17/05/2017 01:55:19

Model name: Conv_BiLSTM
Elapsed training time: 9h:09m:26s

Training set size: 586620
Validation set size: 65179
Validation set fraction: 0.098083
Test set fraction: 0.019152

Hyperparameters
=========================================
Optimizer: adam
Batch size: 128
Max number of epochs: 50
Max sequence length: 100

-----------Training statistics-----------
         acc      loss   val_acc  val_loss
0   0.606945  0.653754  0.632412  0.629771
1   0.635865  0.625613  0.651897  0.609425
2   0.646439  0.613780  0.657236  0.603824
3   0.653396  0.605277  0.664033  0.595137
4   0.658065  0.599580  0.666365  0.592648
5   0.662040  0.594768  0.667884  0.588550
6   0.665565  0.590119  0.672977  0.584131
7   0.668965  0.586180  0.670400  0.584619
8   0.671162  0.583851  0.675371  0.579916
9   0.673388  0.580275  0.676383  0.578247
10  0.675746  0.577821  0.678808  0.576967
11  0.677428  0.575482  0.678317  0.575220
12  0.679840  0.573443  0.678501  0.574658
13  0.681093  0.571672  0.677979  0.574601
14  0.682674  0.569709  0.681784  0.571507
15  0.684181  0.567578  0.682919  0.570776
16  0.684162  0.566456  0.683579  0.569690
17  0.685836  0.564979  0.683871  0.568551
18  0.687096  0.564367  0.684899  0.567885
19  0.687239  0.563017  0.685773  0.568034
20  0.688947  0.561296  0.686218  0.567900
21  0.689690  0.560474  0.687169  0.566870
22  0.689864  0.559876  0.686755  0.566315
23  0.690137  0.559227  0.688565  0.567079
24  0.691296  0.558130  0.686770  0.565582
25  0.692005  0.557408  0.686770  0.565864
26  0.693215  0.556502  0.688028  0.564569
27  0.692798  0.555991  0.689026  0.563017
28  0.694049  0.555396  0.691557  0.563100
29  0.693570  0.555015  0.688166  0.564401
30  0.694448  0.554238  0.690637  0.562274
31  0.694516  0.553941  0.688980  0.562829
32  0.695653  0.552819  0.690176  0.562155
33  0.696093  0.552690  0.690867  0.562063
34  0.696372  0.552284  0.690928  0.561993
35  0.696076  0.552166  0.691450  0.561798
36  0.696686  0.551351  0.689639  0.563298
37  0.697095  0.550742  0.691189  0.561714
38  0.698640  0.550135  0.691649  0.561097
39  0.697629  0.550791  0.692263  0.560971
40  0.698150  0.550569  0.693199  0.560381
41  0.697907  0.549914  0.692447  0.560644
42  0.698774  0.549406  0.690207  0.561453
43  0.698507  0.549435  0.693183  0.560802

Validation PRF
                 Female          Male  Overall Macro  Overall Micro
Precision      0.689558      0.695622       0.692590       0.693183
Recall         0.603775      0.769124       0.686449       0.693183
F-score        0.643821      0.730529       0.687175       0.693183
Support    29935.000000  35244.000000            NaN            NaN

--------------Test results---------------
loss: 0.723360 acc: 0.592130

Test PRF
                Female         Male  Overall Macro  Overall Micro
Precision     0.613186     0.572385       0.592786       0.592127
Recall        0.573424     0.612178       0.592801       0.592127
F-score       0.592639     0.591614       0.592126       0.592127
Support    6585.000000  6142.000000            NaN            NaN

--------------Model Diagram---------------
       InputLayer (None, 100)             InputLayer (None, 100)      
           Lambda (None, 100, 77)             Lambda (None, 100, 77)  
           Conv1D (None, 97, 1024)            Conv1D (None, 97, 1024) 
          Dropout (None, 97, 1024)           Dropout (None, 97, 1024) 
     MaxPooling1D (None, 48, 1024)      MaxPooling1D (None, 48, 1024) 
             LSTM (None, 150)                   LSTM (None, 150)      
                  \________________________________/                  
                                  |                                   
                             Merge (None, 300)                        
                           Dropout (None, 300)                        
                             Dense (None, 2)                          


Model information:
=========================================
 Kernel_size: 4
 Filters: 1024
 Pool length: 2
 LSTM dropout: 0.200000, LSTM recurrent dropout 0.200000
 Conv dropout: 0.500000
 Dense drop1 0.500000
 Dense drop2 0.200000
 No dense layer before softmax
Extra information:
=========================================
 Remove stopwords True
 Lemmatize False
 Remove punctuation False
 Remove emoticons False
 All Internet terms are replaced with tags
 Removed 3449 tweet because they were shorter than threshold